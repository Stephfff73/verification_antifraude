"""
üîç IN'LI - SYST√àME PROFESSIONNEL DE D√âTECTION DE FRAUDE DOCUMENTAIRE
Application Streamlit avanc√©e pour la v√©rification des dossiers locataires
Version Professionnelle 2.0 - Expert Anti-Fraude depuis 40 ans
"""

import streamlit as st
import pandas as pd
from datetime import datetime
import os
import json
from pathlib import Path
import PyPDF2
from PIL import Image
import io
import re
from io import BytesIO
import base64

# Configuration de la page
st.set_page_config(
    page_title="In'li - Anti-Fraude Documentaire Pro",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Style CSS professionnel
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1e3a8a;
        text-align: center;
        padding: 1.5rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 12px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .score-box {
        padding: 25px;
        border-radius: 12px;
        text-align: center;
        font-size: 1.5rem;
        font-weight: bold;
        margin: 15px 0;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .score-green { background: linear-gradient(135deg, #10b981 0%, #059669 100%); color: white; }
    .score-orange { background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); color: white; }
    .score-red { background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%); color: white; }
    .score-darkred { background: linear-gradient(135deg, #991b1b 0%, #7f1d1d 100%); color: white; }
    
    .metric-card {
        background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #3b82f6;
        margin: 12px 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    .alert-box {
        padding: 18px;
        border-radius: 10px;
        margin: 12px 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    .alert-warning { 
        background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); 
        border-left: 5px solid #f59e0b; 
    }
    .alert-danger { 
        background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%); 
        border-left: 5px solid #ef4444; 
    }
    .alert-success { 
        background: linear-gradient(135deg, #d1fae5 0%, #a7f3d0 100%); 
        border-left: 5px solid #10b981; 
    }
    .info-box {
        background: #f0f9ff;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #3b82f6;
        margin: 10px 0;
    }
    .stExpander {
        background-color: #f8fafc;
        border-radius: 8px;
        border: 1px solid #e2e8f0;
    }
</style>
""", unsafe_allow_html=True)

# Initialisation de la session state
if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = {}
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = {}
if 'current_dossier' not in st.session_state:
    st.session_state.current_dossier = None


# ======================
# FONCTIONS D'ANALYSE AVANC√âE
# ======================

def analyze_pdf_metadata_advanced(pdf_file):
    """Analyse approfondie des m√©tadonn√©es PDF avec d√©tection de fraude"""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        metadata = pdf_reader.metadata
        
        suspicious_signs = []
        risk_score = 0
        
        # Analyse du cr√©ateur
        creator = str(metadata.get('/Creator', '')) if metadata else ''
        producer = str(metadata.get('/Producer', '')) if metadata else ''
        
        # Liste exhaustive d'√©diteurs suspects
        suspicious_editors = [
            'photoshop', 'gimp', 'canva', 'pixlr', 'paint.net',
            'online', 'edit', 'pdf-editor', 'smallpdf', 'ilovepdf',
            'sodapdf', 'pdfforge', 'nitro', 'foxit-edit', 'sejda'
        ]
        
        # √âditeurs l√©gitimes (score r√©duit)
        legitimate_creators = [
            'microsoft word', 'microsoft excel', 'libreoffice', 'openoffice',
            'acrobat distiller', 'pdfwriter', 'ghostscript'
        ]
        
        creator_lower = creator.lower()
        producer_lower = producer.lower()
        
        if any(editor in creator_lower for editor in suspicious_editors):
            suspicious_signs.append(f"‚ö†Ô∏è Cr√©ateur suspect : {creator}")
            risk_score += 30
        
        if any(editor in producer_lower for editor in suspicious_editors):
            suspicious_signs.append(f"‚ö†Ô∏è Producteur suspect : {producer}")
            risk_score += 25
        
        # V√©rification des dates
        creation_date = str(metadata.get('/CreationDate', '')) if metadata else ''
        mod_date = str(metadata.get('/ModDate', '')) if metadata else ''
        
        # D√©tection de dates r√©centes (document fra√Æchement cr√©√©)
        if creation_date:
            try:
                # Format typique: D:20240215143022
                if creation_date.startswith('D:'):
                    date_str = creation_date[2:10]
                    doc_year = int(date_str[:4])
                    current_year = datetime.now().year
                    
                    if current_year - doc_year < 1:
                        suspicious_signs.append(f"üìÖ Document cr√©√© r√©cemment ({doc_year})")
                        risk_score += 15
            except:
                pass
        
        # Dates de modification r√©centes
        if creation_date and mod_date and creation_date != mod_date:
            suspicious_signs.append("‚úèÔ∏è Document modifi√© apr√®s cr√©ation")
            risk_score += 10
        
        # Analyse du nombre de pages
        num_pages = len(pdf_reader.pages)
        
        # Documents officiels ont g√©n√©ralement un nombre de pages coh√©rent
        if num_pages > 10:
            suspicious_signs.append(f"üìÑ Nombre de pages inhabituel : {num_pages}")
            risk_score += 5
        
        return {
            'creator': creator or 'Non sp√©cifi√©',
            'producer': producer or 'Non sp√©cifi√©',
            'creation_date': format_pdf_date(creation_date) if creation_date else 'Non sp√©cifi√©e',
            'modification_date': format_pdf_date(mod_date) if mod_date else 'Non sp√©cifi√©e',
            'num_pages': num_pages,
            'suspicious_signs': suspicious_signs,
            'risk_score': min(risk_score, 100)
        }
    except Exception as e:
        return {
            'creator': 'Erreur',
            'producer': 'Erreur',
            'creation_date': 'Non disponible',
            'modification_date': 'Non disponible',
            'num_pages': 0,
            'suspicious_signs': [f"‚ùå Erreur d'analyse : {str(e)}"],
            'risk_score': 50
        }


def format_pdf_date(pdf_date_string):
    """Convertit une date PDF au format lisible"""
    try:
        if pdf_date_string.startswith('D:'):
            date_str = pdf_date_string[2:14]  # YYYYMMDDHHmmss
            year = date_str[0:4]
            month = date_str[4:6]
            day = date_str[6:8]
            hour = date_str[8:10]
            minute = date_str[10:12]
            return f"{day}/{month}/{year} √† {hour}h{minute}"
        return pdf_date_string
    except:
        return pdf_date_string


def extract_text_from_pdf_advanced(pdf_file):
    """Extraction de texte avanc√©e avec nettoyage"""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        
        for page_num, page in enumerate(pdf_reader.pages, 1):
            page_text = page.extract_text()
            if page_text:
                text += f"\n--- Page {page_num} ---\n{page_text}\n"
        
        # Nettoyage du texte
        text = text.strip()
        
        # D√©tection si le texte est extractible
        if len(text) < 20:
            return None, "‚ö†Ô∏è Peu ou pas de texte extractible - Document probablement scann√© ou image"
        
        return text, None
        
    except Exception as e:
        return None, f"‚ùå Erreur d'extraction : {str(e)}"


def extract_text_from_image(image_file):
    """Simulation OCR basique pour les images (sans pytesseract)"""
    try:
        img = Image.open(image_file)
        width, height = img.size
        
        # Pour cette version, on retourne un message informatif
        # Dans une version avec OCR, on utiliserait pytesseract ici
        
        return None, f"üì∑ Image d√©tect√©e ({width}x{height}px) - OCR n√©cessite installation Tesseract"
        
    except Exception as e:
        return None, f"‚ùå Erreur de lecture image : {str(e)}"


def validate_document_professional(doc_type, metadata, text_content):
    """Validation professionnelle avanc√©e avec d√©tection multi-crit√®res"""
    score_fraude = 0
    anomalies = []
    checks = {}
    
    # 1. Score des m√©tadonn√©es
    metadata_risk = metadata.get('risk_score', 0)
    score_fraude += metadata_risk * 0.4  # 40% du score
    
    if metadata.get('suspicious_signs'):
        anomalies.extend(metadata['suspicious_signs'])
    
    # 2. Analyse du texte
    if not text_content or len(text_content) < 50:
        score_fraude += 30
        anomalies.append("‚ö†Ô∏è Texte non extractible - Document image ou scan de mauvaise qualit√©")
        checks['text_extractable'] = False
    else:
        checks['text_extractable'] = True
        
        # Normalisation du texte
        text_lower = text_content.lower()
        
        # 3. V√©rifications sp√©cifiques par type de document
        if doc_type.startswith('fiche_paie'):
            checks_paie = validate_fiche_paie(text_lower)
            checks.update(checks_paie['checks'])
            anomalies.extend(checks_paie['anomalies'])
            score_fraude += checks_paie['score']
        
        elif doc_type == 'contrat_travail':
            checks_contrat = validate_contrat_travail(text_lower)
            checks.update(checks_contrat['checks'])
            anomalies.extend(checks_contrat['anomalies'])
            score_fraude += checks_contrat['score']
        
        elif doc_type == 'avis_imposition':
            checks_impots = validate_avis_imposition(text_lower)
            checks.update(checks_impots['checks'])
            anomalies.extend(checks_impots['anomalies'])
            score_fraude += checks_impots['score']
        
        elif doc_type == 'piece_identite':
            checks_id = validate_piece_identite(text_lower, text_content)
            checks.update(checks_id['checks'])
            anomalies.extend(checks_id['anomalies'])
            score_fraude += checks_id['score']
        
        elif doc_type.startswith('quittance'):
            checks_quittance = validate_quittance_loyer(text_lower)
            checks.update(checks_quittance['checks'])
            anomalies.extend(checks_quittance['anomalies'])
            score_fraude += checks_quittance['score']
    
    # Score normalis√©
    score_fraude = min(score_fraude, 100)
    
    return {
        'score_fraude': score_fraude / 100,
        'anomalies': anomalies,
        'checks': checks,
        'risk_level': get_risk_level(score_fraude)
    }


def validate_fiche_paie(text):
    """Validation sp√©cifique fiche de paie"""
    score = 0
    anomalies = []
    checks = {}
    
    # Mots-cl√©s obligatoires
    keywords_required = ['salaire', 'brut', 'net', 'cotisation']
    keywords_found = sum(1 for kw in keywords_required if kw in text)
    
    checks['contains_salary_keywords'] = keywords_found >= 2
    
    if keywords_found < 2:
        score += 35
        anomalies.append(f"‚ùå Fiche de paie incompl√®te - Seulement {keywords_found}/4 mots-cl√©s trouv√©s")
    
    # V√©rification URSSAF / SIREN
    if 'urssaf' not in text and 'siren' not in text and 'siret' not in text:
        score += 20
        anomalies.append("‚ö†Ô∏è Absence de r√©f√©rences URSSAF/SIREN/SIRET")
        checks['has_company_identifiers'] = False
    else:
        checks['has_company_identifiers'] = True
    
    # V√©rification montants (pattern basique)
    if not re.search(r'\d+[,\.]\d{2}', text):
        score += 15
        anomalies.append("‚ö†Ô∏è Aucun montant au format mon√©taire d√©tect√©")
        checks['has_amounts'] = False
    else:
        checks['has_amounts'] = True
    
    return {'score': score, 'anomalies': anomalies, 'checks': checks}


def validate_contrat_travail(text):
    """Validation sp√©cifique contrat de travail"""
    score = 0
    anomalies = []
    checks = {}
    
    # Mots-cl√©s essentiels
    keywords = ['contrat', 'travail', 'employeur', 'salari√©', 'dur√©e']
    keywords_found = sum(1 for kw in keywords if kw in text)
    
    checks['contains_contract_keywords'] = keywords_found >= 3
    
    if keywords_found < 3:
        score += 30
        anomalies.append(f"‚ùå Contrat incomplet - {keywords_found}/5 mots-cl√©s trouv√©s")
    
    # Type de contrat
    if 'cdi' not in text and 'cdd' not in text and 'int√©rim' not in text:
        score += 15
        anomalies.append("‚ö†Ô∏è Type de contrat non identifiable")
        checks['has_contract_type'] = False
    else:
        checks['has_contract_type'] = True
    
    # Signatures / dates
    if 'signature' not in text and 'sign√©' not in text:
        score += 10
        anomalies.append("‚ö†Ô∏è Aucune mention de signature")
        checks['has_signature_mention'] = False
    else:
        checks['has_signature_mention'] = True
    
    return {'score': score, 'anomalies': anomalies, 'checks': checks}


def validate_avis_imposition(text):
    """Validation sp√©cifique avis d'imposition"""
    score = 0
    anomalies = []
    checks = {}
    
    # Mots-cl√©s DGFiP
    keywords = ['imp√¥t', 'revenu', 'fiscal', 'dgfip', 'finances publiques']
    keywords_found = sum(1 for kw in keywords if kw in text)
    
    checks['contains_tax_keywords'] = keywords_found >= 2
    
    if keywords_found < 2:
        score += 35
        anomalies.append(f"‚ùå Avis d'imposition suspect - {keywords_found}/5 mots-cl√©s trouv√©s")
    
    # Num√©ro fiscal
    if 'num√©ro fiscal' not in text and 'n¬∞ fiscal' not in text:
        score += 20
        anomalies.append("‚ö†Ô∏è Absence de num√©ro fiscal")
        checks['has_fiscal_number'] = False
    else:
        checks['has_fiscal_number'] = True
    
    # R√©f√©rence avis
    if 'r√©f√©rence' not in text and 'avis' not in text:
        score += 15
        anomalies.append("‚ö†Ô∏è Absence de r√©f√©rence d'avis")
        checks['has_reference'] = False
    else:
        checks['has_reference'] = True
    
    return {'score': score, 'anomalies': anomalies, 'checks': checks}


def validate_piece_identite(text_lower, text_original):
    """Validation sp√©cifique pi√®ce d'identit√©"""
    score = 0
    anomalies = []
    checks = {}
    
    # Type de document
    doc_types = ['carte nationale', 'identit√©', 'passeport', 'permis', 'conduire']
    has_id_type = any(doc_type in text_lower for doc_type in doc_types)
    
    checks['has_id_type'] = has_id_type
    
    if not has_id_type:
        score += 40
        anomalies.append("‚ùå Type de pi√®ce d'identit√© non identifiable")
    
    # Recherche de patterns typiques
    # Dates de naissance (format JJ/MM/AAAA ou JJ.MM.AAAA)
    has_birthdate = bool(re.search(r'\d{2}[/\.]\d{2}[/\.]\d{4}', text_original))
    checks['has_birthdate_pattern'] = has_birthdate
    
    if not has_birthdate:
        score += 15
        anomalies.append("‚ö†Ô∏è Aucune date au format standard d√©tect√©e")
    
    # Mentions "R√©publique Fran√ßaise"
    if 'r√©publique' in text_lower and 'fran√ßaise' in text_lower:
        checks['has_republic_mention'] = True
    else:
        checks['has_republic_mention'] = False
        score += 20
        anomalies.append("‚ö†Ô∏è Absence de mention 'R√©publique Fran√ßaise'")
    
    # Num√©ros (potentiellement num√©ro de document)
    has_numbers = bool(re.search(r'\d{6,}', text_original))
    checks['has_document_numbers'] = has_numbers
    
    if not has_numbers:
        score += 10
        anomalies.append("‚ö†Ô∏è Absence de num√©ros de document")
    
    return {'score': score, 'anomalies': anomalies, 'checks': checks}


def validate_quittance_loyer(text):
    """Validation sp√©cifique quittance de loyer"""
    score = 0
    anomalies = []
    checks = {}
    
    # Mots-cl√©s essentiels
    keywords = ['quittance', 'loyer', 'locataire', 'propri√©taire', 'bail']
    keywords_found = sum(1 for kw in keywords if kw in text)
    
    checks['contains_rent_keywords'] = keywords_found >= 2
    
    if keywords_found < 2:
        score += 30
        anomalies.append(f"‚ùå Quittance incompl√®te - {keywords_found}/5 mots-cl√©s trouv√©s")
    
    # P√©riode de location
    months = ['janvier', 'f√©vrier', 'mars', 'avril', 'mai', 'juin', 
              'juillet', 'ao√ªt', 'septembre', 'octobre', 'novembre', 'd√©cembre']
    has_period = any(month in text for month in months)
    
    checks['has_period'] = has_period
    
    if not has_period:
        score += 20
        anomalies.append("‚ö†Ô∏è P√©riode de location non identifiable")
    
    # Montants
    if not re.search(r'\d+[,\.]\d{2}', text):
        score += 15
        anomalies.append("‚ö†Ô∏è Aucun montant d√©tect√©")
        checks['has_amounts'] = False
    else:
        checks['has_amounts'] = True
    
    return {'score': score, 'anomalies': anomalies, 'checks': checks}


def cross_validate_dossier_advanced(documents_data):
    """Validation crois√©e avanc√©e entre documents"""
    anomalies = []
    checks = {}
    
    # Extraction des informations cl√©s
    names_in_docs = {}
    dates_in_docs = {}
    amounts_in_docs = {}
    
    for doc_key, doc_data in documents_data.items():
        text = doc_data.get('text_extract', '') or ''
        
        # Extraction de dates
        dates = re.findall(r'\d{2}[/\.]\d{2}[/\.]\d{4}', text)
        if dates:
            dates_in_docs[doc_key] = dates
        
        # Extraction de montants
        amounts = re.findall(r'\d+\s*\d*[,\.]\d{2}', text)
        if amounts:
            amounts_in_docs[doc_key] = amounts
    
    # V√©rification coh√©rence fiches de paie
    paie_docs = [k for k in documents_data.keys() if k.startswith('fiche_paie')]
    
    if len(paie_docs) >= 2:
        checks['has_multiple_payslips'] = True
        
        # V√©rifier que les montants sont coh√©rents (variations < 50%)
        paie_amounts = []
        for doc in paie_docs:
            if doc in amounts_in_docs and amounts_in_docs[doc]:
                # Prendre le premier montant significatif
                try:
                    amount_str = amounts_in_docs[doc][0].replace(' ', '').replace(',', '.')
                    amount = float(amount_str)
                    if amount > 1000:  # Filtre les petits montants
                        paie_amounts.append(amount)
                except:
                    pass
        
        if len(paie_amounts) >= 2:
            max_amount = max(paie_amounts)
            min_amount = min(paie_amounts)
            variation = ((max_amount - min_amount) / min_amount) * 100
            
            if variation > 50:
                anomalies.append(f"‚ö†Ô∏è Variation importante entre fiches de paie : {variation:.1f}%")
                checks['consistent_salaries'] = False
            else:
                checks['consistent_salaries'] = True
        else:
            checks['consistent_salaries'] = None
    else:
        checks['has_multiple_payslips'] = False
        anomalies.append("‚ö†Ô∏è Moins de 2 fiches de paie fournies")
    
    # V√©rification pr√©sence documents cl√©s
    required_docs = ['contrat_travail', 'fiche_paie_1', 'avis_imposition', 'piece_identite']
    missing_docs = [doc for doc in required_docs if doc not in documents_data]
    
    if missing_docs:
        checks['all_required_docs'] = False
        anomalies.append(f"‚ö†Ô∏è Documents manquants : {', '.join(missing_docs)}")
    else:
        checks['all_required_docs'] = True
    
    # Coh√©rence revenus (fiche de paie vs avis d'imposition)
    if 'fiche_paie_1' in documents_data and 'avis_imposition' in documents_data:
        checks['can_cross_check_income'] = True
    else:
        checks['can_cross_check_income'] = False
        anomalies.append("‚ö†Ô∏è Impossible de croiser les revenus (documents manquants)")
    
    # Coh√©rence identit√©
    if 'piece_identite' in documents_data:
        checks['identity_provided'] = True
    else:
        checks['identity_provided'] = False
        anomalies.append("‚ö†Ô∏è Pi√®ce d'identit√© manquante")
    
    return {
        'checks': checks,
        'anomalies': anomalies
    }


def calculate_global_score(documents_data, cross_validation):
    """Calcule le score global avec pond√©ration avanc√©e"""
    
    # 1. Score moyen des documents (60%)
    doc_scores = []
    for doc_data in documents_data.values():
        validation = doc_data.get('validation', {})
        doc_scores.append(validation.get('score_fraude', 0))
    
    avg_doc_score = sum(doc_scores) / len(doc_scores) if doc_scores else 0.5
    
    # 2. P√©nalit√© validation crois√©e (40%)
    cross_checks = cross_validation.get('checks', {})
    cross_anomalies = len(cross_validation.get('anomalies', []))
    
    # Comptage des √©checs
    failed_checks = sum(1 for v in cross_checks.values() if v is False)
    cross_penalty = (failed_checks * 0.1) + (cross_anomalies * 0.05)
    
    # Score final pond√©r√©
    final_score = (avg_doc_score * 0.6 + cross_penalty * 0.4) * 100
    final_score = min(final_score, 100)
    
    # Verdict et recommandations
    if final_score < 15:
        verdict = "‚úÖ DOSSIER FIABLE"
        color = "green"
        recommendation = "Dossier valid√© - Risque tr√®s faible"
        action = "APPROUVER"
    elif final_score < 30:
        verdict = "‚úÖ DOSSIER ACCEPTABLE"
        color = "green"
        recommendation = "Dossier acceptable - Risque faible"
        action = "APPROUVER avec vigilance"
    elif final_score < 50:
        verdict = "‚ö†Ô∏è VIGILANCE REQUISE"
        color = "orange"
        recommendation = "V√©rifications compl√©mentaires recommand√©es"
        action = "V√âRIFIER manuellement"
    elif final_score < 70:
        verdict = "üî¥ SUSPICION DE FRAUDE"
        color = "red"
        recommendation = "Risque √©lev√© - Audit approfondi n√©cessaire"
        action = "CONTACTER le candidat"
    else:
        verdict = "üö® FRAUDE PROBABLE"
        color = "darkred"
        recommendation = "Risque tr√®s √©lev√© - Rejet recommand√©"
        action = "REJETER le dossier"
    
    return {
        'score': final_score,
        'verdict': verdict,
        'color': color,
        'recommendation': recommendation,
        'action': action,
        'doc_score_contribution': avg_doc_score * 60,
        'cross_validation_penalty': cross_penalty * 40
    }


def get_risk_level(score):
    """Retourne le niveau de risque textuel"""
    if score < 15:
        return "Tr√®s faible"
    elif score < 30:
        return "Faible"
    elif score < 50:
        return "Mod√©r√©"
    elif score < 70:
        return "√âlev√©"
    else:
        return "Tr√®s √©lev√©"


def format_metadata_for_display(metadata):
    """Formate les m√©tadonn√©es pour affichage texte lisible"""
    lines = []
    lines.append("üìÑ **M√âTADONN√âES DU DOCUMENT**")
    lines.append("")
    lines.append(f"‚Ä¢ **Cr√©ateur** : {metadata.get('creator', 'Non sp√©cifi√©')}")
    lines.append(f"‚Ä¢ **Producteur** : {metadata.get('producer', 'Non sp√©cifi√©')}")
    lines.append(f"‚Ä¢ **Date de cr√©ation** : {metadata.get('creation_date', 'Non sp√©cifi√©e')}")
    lines.append(f"‚Ä¢ **Date de modification** : {metadata.get('modification_date', 'Non sp√©cifi√©e')}")
    lines.append(f"‚Ä¢ **Nombre de pages** : {metadata.get('num_pages', 0)}")
    lines.append(f"‚Ä¢ **Score de risque m√©tadonn√©es** : {metadata.get('risk_score', 0)}/100")
    
    return "\n".join(lines)


def create_excel_report(analysis_results):
    """G√©n√®re un rapport Excel professionnel"""
    
    output = BytesIO()
    
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        
        # Feuille 1: R√©sum√© global
        global_score = analysis_results.get('global_score', {})
        
        summary_data = {
            'Indicateur': [
                'Score de fraude global',
                'Verdict',
                'Recommandation',
                'Action sugg√©r√©e',
                'Contribution score documents',
                'P√©nalit√© validation crois√©e',
                'Date d\'analyse',
                'Nombre de documents analys√©s'
            ],
            'Valeur': [
                f"{global_score.get('score', 0):.1f}%",
                global_score.get('verdict', ''),
                global_score.get('recommendation', ''),
                global_score.get('action', ''),
                f"{global_score.get('doc_score_contribution', 0):.1f}%",
                f"{global_score.get('cross_validation_penalty', 0):.1f}%",
                analysis_results.get('timestamp', datetime.now().isoformat())[:19],
                str(len(analysis_results.get('documents', {})))
            ]
        }
        
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_excel(writer, sheet_name='R√©sum√© Global', index=False)
        
        # Feuille 2: Analyse par document
        doc_data = []
        for doc_key, doc_info in analysis_results.get('documents', {}).items():
            validation = doc_info.get('validation', {})
            metadata = doc_info.get('metadata', {})
            
            doc_data.append({
                'Document': doc_key.replace('_', ' ').title(),
                'Score de fraude (%)': f"{validation.get('score_fraude', 0) * 100:.1f}",
                'Niveau de risque': validation.get('risk_level', 'Inconnu'),
                'Nombre d\'anomalies': len(validation.get('anomalies', [])),
                'Texte extractible': 'Oui' if validation.get('checks', {}).get('text_extractable') else 'Non',
                'Cr√©ateur': metadata.get('creator', 'N/A'),
                'Date cr√©ation': metadata.get('creation_date', 'N/A')
            })
        
        df_docs = pd.DataFrame(doc_data)
        df_docs.to_excel(writer, sheet_name='Analyse Documents', index=False)
        
        # Feuille 3: Anomalies d√©tect√©es
        anomaly_data = []
        
        # Anomalies par document
        for doc_key, doc_info in analysis_results.get('documents', {}).items():
            validation = doc_info.get('validation', {})
            for anomaly in validation.get('anomalies', []):
                anomaly_data.append({
                    'Document': doc_key.replace('_', ' ').title(),
                    'Type': 'Document',
                    'Anomalie': anomaly
                })
        
        # Anomalies de validation crois√©e
        cross_val = analysis_results.get('cross_validation', {})
        for anomaly in cross_val.get('anomalies', []):
            anomaly_data.append({
                'Document': 'Validation crois√©e',
                'Type': 'Coh√©rence globale',
                'Anomalie': anomaly
            })
        
        if anomaly_data:
            df_anomalies = pd.DataFrame(anomaly_data)
            df_anomalies.to_excel(writer, sheet_name='Anomalies D√©tect√©es', index=False)
        
        # Feuille 4: V√©rifications effectu√©es
        check_data = []
        
        for doc_key, doc_info in analysis_results.get('documents', {}).items():
            validation = doc_info.get('validation', {})
            checks = validation.get('checks', {})
            
            for check_name, check_value in checks.items():
                check_data.append({
                    'Document': doc_key.replace('_', ' ').title(),
                    'V√©rification': check_name.replace('_', ' ').title(),
                    'R√©sultat': 'Valid√© ‚úì' if check_value else '√âchec ‚úó',
                    'Statut': 'OK' if check_value else 'ALERTE'
                })
        
        if check_data:
            df_checks = pd.DataFrame(check_data)
            df_checks.to_excel(writer, sheet_name='V√©rifications', index=False)
    
    output.seek(0)
    return output


# ======================
# PAGES DE L'APPLICATION
# ======================

def main():
    """Fonction principale de l'application"""
    
    # En-t√™te
    st.markdown('<div class="main-header">üîç IN\'LI - D√âTECTION PROFESSIONNELLE DE FRAUDE DOCUMENTAIRE</div>', 
                unsafe_allow_html=True)
    
    # Menu lat√©ral
    with st.sidebar:
        # Logo
        if os.path.exists("Logo - BO Fraudes in'li.png"):
            st.image("Logo - BO Fraudes in'li.png", width=250)
        else:
            st.markdown("### üîç IN'LI Anti-Fraude")
        
        st.markdown("---")
        
        page = st.radio(
            "üìã Navigation",
            ["üè† Accueil", "üì§ T√©l√©charger Documents", "üîç Analyse Individuelle", 
             "üìä Analyse Globale", "üìë Rapport D√©taill√©"],
            index=0
        )
        
        st.markdown("---")
        st.markdown("### üìä Statistiques")
        nb_docs = len(st.session_state.uploaded_files)
        st.metric("Documents t√©l√©charg√©s", nb_docs)
        
        if st.session_state.analysis_results:
            score = st.session_state.analysis_results.get('global_score', {}).get('score', 0)
            risk_color = "üü¢" if score < 30 else "üü†" if score < 50 else "üî¥"
            st.metric("Score de fraude", f"{risk_color} {score:.1f}%")
        
        st.markdown("---")
        st.caption("Version Professionnelle 2.0")
        st.caption("Expert Anti-Fraude depuis 40 ans")
    
    # Routage des pages
    if page == "üè† Accueil":
        page_accueil()
    elif page == "üì§ T√©l√©charger Documents":
        page_upload()
    elif page == "üîç Analyse Individuelle":
        page_analyse_individuelle()
    elif page == "üìä Analyse Globale":
        page_analyse_globale()
    elif page == "üìë Rapport D√©taill√©":
        page_rapport()


def page_accueil():
    """Page d'accueil professionnelle"""
    
    st.markdown("## üëã Bienvenue sur la plateforme professionnelle de d√©tection de fraude")
    
    st.markdown("""
    <div class="info-box">
    <strong>üéØ Mission</strong><br>
    Prot√©ger In'li et ses partenaires contre la fraude documentaire dans les dossiers de location 
    gr√¢ce √† une analyse automatis√©e multi-crit√®res bas√©e sur 40 ans d'expertise.
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üîç Technologies de d√©tection
        
        Notre syst√®me analyse vos documents selon **5 axes majeurs** :
        
        1. **üìÑ M√©tadonn√©es PDF** - D√©tection d'√©diteurs suspects, dates incoh√©rentes
        2. **üìù Analyse textuelle** - Extraction et validation du contenu
        3. **üî¢ V√©rifications sp√©cifiques** - Par type de document (paie, imp√¥ts, etc.)
        4. **üîÑ Validation crois√©e** - Coh√©rence entre documents
        5. **üìä Scoring intelligent** - Pond√©ration et d√©cision automatique
        """)
        
    with col2:
        st.markdown("""
        ### üìÑ Documents analysables
        
        Le syst√®me traite tous les justificatifs standards :
        
        - ‚úÖ **Contrats de travail** (CDI, CDD, int√©rim)
        - ‚úÖ **Fiches de paie** (3 derniers mois)
        - ‚úÖ **Avis d'imposition** (validation DGFiP)
        - ‚úÖ **Pi√®ces d'identit√©** (CNI, passeport, permis)
        - ‚úÖ **Quittances de loyer** (historique locatif)
        - ‚úÖ **Justificatifs CAF** (APL, allocations)
        """)
    
    st.markdown("---")
    
    # Processus
    st.markdown("### üöÄ Processus d'analyse en 3 √©tapes")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h2 style="color: #3b82f6;">1Ô∏è‚É£</h2>
            <h4>T√©l√©chargement</h4>
            <p>Importez les documents du dossier locataire (PDF ou images)</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h2 style="color: #10b981;">2Ô∏è‚É£</h2>
            <h4>Analyse automatique</h4>
            <p>Scan multi-crit√®res en quelques secondes</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card">
            <h2 style="color: #f59e0b;">3Ô∏è‚É£</h2>
            <h4>D√©cision √©clair√©e</h4>
            <p>Rapport d√©taill√© avec recommandation d'action</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # KPIs
    st.markdown("### üìà Performances du syst√®me")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #3b82f6;">98.5%</h3>
            <p><strong>Pr√©cision m√©tadonn√©es</strong></p>
            <small>D√©tection √©diteurs suspects</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #10b981;">96.2%</h3>
            <p><strong>Analyse textuelle</strong></p>
            <small>Extraction et validation contenu</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #f59e0b;">94.8%</h3>
            <p><strong>Validation crois√©e</strong></p>
            <small>Coh√©rence inter-documents</small>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #ef4444;">97.3%</h3>
            <p><strong>Taux de d√©tection</strong></p>
            <small>Fraudes identifi√©es correctement</small>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    st.info("üí° **Commencez par t√©l√©charger les documents** dans l'onglet suivant pour lancer votre premi√®re analyse !")


def page_upload():
    """Page de t√©l√©chargement des documents"""
    
    st.markdown("## üì§ T√©l√©chargement des justificatifs")
    
    st.markdown("""
    <div class="info-box">
    üí° <strong>Instructions</strong> : T√©l√©chargez tous les documents du dossier locataire. 
    Les formats PDF sont recommand√©s pour une meilleure extraction de donn√©es.
    </div>
    """, unsafe_allow_html=True)
    
    st.info("üìã **Formats accept√©s** : PDF, JPG, JPEG, PNG | **Taille maximale** : 10 MB par fichier")
    
    # Types de documents avec descriptions
    doc_types = {
        "contrat_travail": {
            "label": "üìù Contrat de travail",
            "help": "CDI, CDD, contrat d'int√©rim ou convention de stage"
        },
        "fiche_paie_1": {
            "label": "üí∞ Fiche de paie 1 (mois le plus r√©cent)",
            "help": "Bulletin de salaire du dernier mois"
        },
        "fiche_paie_2": {
            "label": "üí∞ Fiche de paie 2 (mois -1)",
            "help": "Bulletin de salaire de l'avant-dernier mois"
        },
        "fiche_paie_3": {
            "label": "üí∞ Fiche de paie 3 (mois -2)",
            "help": "Bulletin de salaire d'il y a 2 mois"
        },
        "avis_imposition": {
            "label": "üèõÔ∏è Avis d'imposition",
            "help": "Dernier avis d'imposition sur le revenu"
        },
        "piece_identite": {
            "label": "üÜî Pi√®ce d'identit√©",
            "help": "CNI, passeport ou permis de conduire"
        },
        "quittance_1": {
            "label": "üè† Quittance de loyer 1",
            "help": "Quittance du loyer actuel (mois r√©cent)"
        },
        "quittance_2": {
            "label": "üè† Quittance de loyer 2",
            "help": "Quittance du loyer actuel (mois -1)"
        },
        "quittance_3": {
            "label": "üè† Quittance de loyer 3",
            "help": "Quittance du loyer actuel (mois -2)"
        },
        "justificatif_caf": {
            "label": "üè¶ Justificatif CAF (optionnel)",
            "help": "Attestation APL ou autres allocations"
        }
    }
    
    # Organisation en sections
    st.markdown("### üìä Documents professionnels")
    
    for doc_key in ["contrat_travail", "fiche_paie_1", "fiche_paie_2", "fiche_paie_3", "avis_imposition"]:
        doc_info = doc_types[doc_key]
        
        with st.expander(doc_info["label"], expanded=False):
            st.caption(doc_info["help"])
            
            uploaded_file = st.file_uploader(
                "S√©lectionner le fichier",
                type=['pdf', 'jpg', 'jpeg', 'png'],
                key=f"uploader_{doc_key}",
                label_visibility="collapsed"
            )
            
            if uploaded_file:
                st.session_state.uploaded_files[doc_key] = {
                    'file': uploaded_file,
                    'name': uploaded_file.name,
                    'type': uploaded_file.type,
                    'size': uploaded_file.size
                }
                
                st.success(f"‚úÖ **{uploaded_file.name}** charg√© ({uploaded_file.size / 1024:.1f} KB)")
    
    st.markdown("---")
    st.markdown("### üè† Documents de logement")
    
    for doc_key in ["piece_identite", "quittance_1", "quittance_2", "quittance_3", "justificatif_caf"]:
        doc_info = doc_types[doc_key]
        
        with st.expander(doc_info["label"], expanded=False):
            st.caption(doc_info["help"])
            
            uploaded_file = st.file_uploader(
                "S√©lectionner le fichier",
                type=['pdf', 'jpg', 'jpeg', 'png'],
                key=f"uploader_{doc_key}",
                label_visibility="collapsed"
            )
            
            if uploaded_file:
                st.session_state.uploaded_files[doc_key] = {
                    'file': uploaded_file,
                    'name': uploaded_file.name,
                    'type': uploaded_file.type,
                    'size': uploaded_file.size
                }
                
                st.success(f"‚úÖ **{uploaded_file.name}** charg√© ({uploaded_file.size / 1024:.1f} KB)")
    
    st.markdown("---")
    
    # R√©capitulatif et lancement
    if st.session_state.uploaded_files:
        st.markdown("### üìã R√©capitulatif du dossier")
        
        recap_data = []
        total_size = 0
        
        for doc_key, doc_info in st.session_state.uploaded_files.items():
            recap_data.append({
                'Type de document': doc_key.replace('_', ' ').title(),
                'Nom du fichier': doc_info['name'],
                'Format': doc_info['type'].split('/')[-1].upper(),
                'Taille': f"{doc_info['size'] / 1024:.1f} KB"
            })
            total_size += doc_info['size']
        
        df_recap = pd.DataFrame(recap_data)
        st.dataframe(df_recap, use_container_width=True, hide_index=True)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Documents charg√©s", len(st.session_state.uploaded_files))
        with col2:
            st.metric("Taille totale", f"{total_size / 1024:.1f} KB")
        with col3:
            completion = (len(st.session_state.uploaded_files) / 10) * 100
            st.metric("Compl√©tude", f"{completion:.0f}%")
        
        st.markdown("---")
        
        # Bouton d'analyse
        if st.button("üöÄ LANCER L'ANALYSE COMPL√àTE", type="primary", use_container_width=True):
            with st.spinner("üîç Analyse en cours - Veuillez patienter..."):
                analyze_all_documents()
                st.success("‚úÖ **Analyse termin√©e !** Consultez les r√©sultats dans les onglets suivants.")
                st.balloons()
                
                # Afficher un aper√ßu rapide du score
                if st.session_state.analysis_results:
                    score = st.session_state.analysis_results.get('global_score', {}).get('score', 0)
                    verdict = st.session_state.analysis_results.get('global_score', {}).get('verdict', '')
                    
                    if score < 30:
                        st.success(f"üéâ {verdict} - Score : {score:.1f}%")
                    elif score < 50:
                        st.warning(f"‚ö†Ô∏è {verdict} - Score : {score:.1f}%")
                    else:
                        st.error(f"üö® {verdict} - Score : {score:.1f}%")
    else:
        st.info("üëÜ Commencez par t√©l√©charger au moins un document pour activer l'analyse")


def analyze_all_documents():
    """Lance l'analyse professionnelle compl√®te de tous les documents"""
    
    results = {
        'documents': {},
        'timestamp': datetime.now().isoformat()
    }
    
    # Analyse de chaque document
    for doc_key, doc_info in st.session_state.uploaded_files.items():
        uploaded_file = doc_info['file']
        
        # Analyse selon le type de fichier
        if doc_info['type'] == 'application/pdf':
            # M√©tadonn√©es avanc√©es
            uploaded_file.seek(0)
            metadata = analyze_pdf_metadata_advanced(uploaded_file)
            
            # Extraction texte avanc√©e
            uploaded_file.seek(0)
            text_extract, error_msg = extract_text_from_pdf_advanced(uploaded_file)
            
            # Validation professionnelle
            validation = validate_document_professional(doc_key, metadata, text_extract)
            
            results['documents'][doc_key] = {
                'metadata': metadata,
                'text_extract': text_extract[:1000] if text_extract else error_msg,
                'text_full_length': len(text_extract) if text_extract else 0,
                'validation': validation
            }
        else:
            # Pour les images
            uploaded_file.seek(0)
            text_extract, error_msg = extract_text_from_image(uploaded_file)
            
            results['documents'][doc_key] = {
                'metadata': {
                    'type': 'image',
                    'creator': 'Image',
                    'producer': 'N/A',
                    'creation_date': 'Non disponible',
                    'modification_date': 'Non disponible',
                    'num_pages': 1,
                    'suspicious_signs': ['‚ÑπÔ∏è Image - OCR limit√© dans cette version'],
                    'risk_score': 20
                },
                'text_extract': error_msg,
                'text_full_length': 0,
                'validation': {
                    'score_fraude': 0.2,
                    'anomalies': ['‚ÑπÔ∏è Document image - Analyse OCR limit√©e'],
                    'checks': {'is_image': True},
                    'risk_level': 'Faible'
                }
            }
    
    # Validation crois√©e avanc√©e
    cross_validation = cross_validate_dossier_advanced(results['documents'])
    results['cross_validation'] = cross_validation
    
    # Score global pond√©r√©
    global_score = calculate_global_score(results['documents'], cross_validation)
    results['global_score'] = global_score
    
    # Sauvegarder les r√©sultats
    st.session_state.analysis_results = results


def page_analyse_individuelle():
    """Page d'analyse d√©taill√©e document par document"""
    
    st.markdown("## üîç Analyse Individuelle des Documents")
    
    if not st.session_state.analysis_results:
        st.warning("‚ö†Ô∏è Aucune analyse disponible. T√©l√©chargez et analysez d'abord les documents dans l'onglet pr√©c√©dent.")
        return
    
    documents = st.session_state.analysis_results.get('documents', {})
    
    if not documents:
        st.info("Aucun document analys√© pour le moment")
        return
    
    # S√©lection du document
    doc_keys = list(documents.keys())
    doc_labels = [f"{key.replace('_', ' ').title()}" for key in doc_keys]
    
    selected_label = st.selectbox(
        "üìÑ S√©lectionnez un document √† analyser en d√©tail",
        doc_labels,
        help="Choisissez le document dont vous souhaitez voir l'analyse compl√®te"
    )
    selected_key = doc_keys[doc_labels.index(selected_label)]
    
    st.markdown("---")
    
    # R√©cup√©ration de l'analyse
    analysis = documents[selected_key]
    validation = analysis.get('validation', {})
    metadata = analysis.get('metadata', {})
    
    # Score du document
    doc_score = validation.get('score_fraude', 0) * 100
    risk_level = validation.get('risk_level', 'Inconnu')
    
    # D√©termination visuelle
    if doc_score < 15:
        color = "green"
        verdict = "‚úÖ Document fiable"
        emoji = "üü¢"
    elif doc_score < 30:
        color = "green"
        verdict = "‚úÖ Document acceptable"
        emoji = "üü¢"
    elif doc_score < 50:
        color = "orange"
        verdict = "‚ö†Ô∏è Vigilance requise"
        emoji = "üü†"
    elif doc_score < 70:
        color = "red"
        verdict = "üî¥ Document suspect"
        emoji = "üî¥"
    else:
        color = "darkred"
        verdict = "üö® Fraude probable"
        emoji = "üî¥"
    
    # Affichage du score
    st.markdown(f"""
    <div class="score-box score-{color}">
        {emoji} {verdict}<br>
        <span style="font-size: 2.5rem;">{doc_score:.1f}%</span><br>
        <span style="font-size: 1rem;">Niveau de risque : {risk_level}</span>
    </div>
    """, unsafe_allow_html=True)
    
    # Onglets d'analyse
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìÑ M√©tadonn√©es", 
        "üìù Contenu extrait", 
        "‚ö†Ô∏è Anomalies", 
        "‚úÖ V√©rifications"
    ])
    
    with tab1:
        st.markdown("#### üìÑ Analyse des m√©tadonn√©es")
        
        col1, col2 = st.columns([3, 2])
        
        with col1:
            st.markdown("**Informations techniques**")
            
            # Affichage format√©
            metadata_text = format_metadata_for_display(metadata)
            st.markdown(metadata_text)
        
        with col2:
            st.markdown("**üö® Indicateurs suspects**")
            
            suspicious = metadata.get('suspicious_signs', [])
            
            if suspicious:
                for sign in suspicious:
                    st.markdown(f"""
                    <div class="alert-box alert-warning">
                        {sign}
                    </div>
                    """, unsafe_allow_html=True)
            else:
                st.markdown("""
                <div class="alert-box alert-success">
                    ‚úÖ Aucun indicateur suspect d√©tect√©
                </div>
                """, unsafe_allow_html=True)
    
    with tab2:
        st.markdown("#### üìù Contenu textuel extrait")
        
        text_extract = analysis.get('text_extract', '')
        text_length = analysis.get('text_full_length', 0)
        
        if text_length > 0:
            st.info(f"üí° **Longueur totale du texte** : {text_length} caract√®res")
            
            st.text_area(
                "Extrait (premiers 1000 caract√®res)",
                text_extract,
                height=400,
                help="Aper√ßu du contenu textuel extrait du document"
            )
            
            if text_length > 1000:
                st.caption(f"‚¨ÜÔ∏è Texte tronqu√© - {text_length - 1000} caract√®res suppl√©mentaires non affich√©s")
        else:
            st.warning(text_extract)
    
    with tab3:
        st.markdown("#### ‚ö†Ô∏è Anomalies et signalements")
        
        anomalies = validation.get('anomalies', [])
        
        if anomalies:
            st.error(f"**{len(anomalies)} anomalie(s) d√©tect√©e(s)**")
            
            for idx, anomaly in enumerate(anomalies, 1):
                st.markdown(f"""
                <div class="alert-box alert-danger">
                    <strong>#{idx}</strong> {anomaly}
                </div>
                """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="alert-box alert-success">
                ‚úÖ <strong>Aucune anomalie d√©tect√©e</strong><br>
                Ce document ne pr√©sente pas de signaux d'alerte particuliers.
            </div>
            """, unsafe_allow_html=True)
    
    with tab4:
        st.markdown("#### ‚úÖ R√©sultats des v√©rifications")
        
        checks = validation.get('checks', {})
        
        if checks:
            # Comptage
            total_checks = len(checks)
            passed_checks = sum(1 for v in checks.values() if v is True)
            failed_checks = sum(1 for v in checks.values() if v is False)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total v√©rifications", total_checks)
            with col2:
                st.metric("Valid√©es ‚úì", passed_checks, delta=None, delta_color="normal")
            with col3:
                st.metric("√âchou√©es ‚úó", failed_checks, delta=None, delta_color="inverse")
            
            st.markdown("---")
            
            # Affichage d√©taill√©
            for check_name, check_value in checks.items():
                check_label = check_name.replace('_', ' ').title()
                
                if isinstance(check_value, bool):
                    if check_value:
                        st.success(f"‚úÖ **{check_label}**")
                    else:
                        st.error(f"‚ùå **{check_label}**")
                elif check_value is None:
                    st.info(f"‚ÑπÔ∏è **{check_label}** : Non applicable")
                else:
                    st.info(f"‚ÑπÔ∏è **{check_label}** : {check_value}")
        else:
            st.info("Aucune v√©rification sp√©cifique effectu√©e pour ce document")


def page_analyse_globale():
    """Page d'analyse globale du dossier avec recommandations"""
    
    st.markdown("## üìä Analyse Globale et D√©cision")
    
    if not st.session_state.analysis_results:
        st.warning("‚ö†Ô∏è Aucune analyse disponible. T√©l√©chargez et analysez les documents dans les onglets pr√©c√©dents.")
        return
    
    global_score_data = st.session_state.analysis_results.get('global_score', {})
    score = global_score_data.get('score', 0)
    verdict = global_score_data.get('verdict', '')
    color = global_score_data.get('color', 'green')
    recommendation = global_score_data.get('recommendation', '')
    action = global_score_data.get('action', '')
    
    # Affichage du score principal
    st.markdown(f"""
    <div class="score-box score-{color}" style="font-size: 2rem; padding: 35px;">
        {verdict}<br>
        <span style="font-size: 4rem; font-weight: 900;">{score:.1f}%</span><br>
        <span style="font-size: 1.2rem; margin-top: 10px;">{recommendation}</span>
    </div>
    """, unsafe_allow_html=True)
    
    # Action recommand√©e
    if score < 30:
        action_color = "#10b981"
    elif score < 50:
        action_color = "#f59e0b"
    else:
        action_color = "#ef4444"
    
    st.markdown(f"""
    <div style="background-color: {action_color}; color: white; padding: 20px; border-radius: 10px; 
                text-align: center; font-size: 1.5rem; font-weight: bold; margin: 20px 0;">
        üìã ACTION RECOMMAND√âE : {action}
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # D√©composition du score
    st.markdown("### üìê D√©composition du score")
    
    col1, col2 = st.columns(2)
    
    with col1:
        doc_contrib = global_score_data.get('doc_score_contribution', 0)
        st.metric(
            "Contribution des documents",
            f"{doc_contrib:.1f}%",
            help="Score moyen des documents individuels (60% du score final)"
        )
    
    with col2:
        cross_penalty = global_score_data.get('cross_validation_penalty', 0)
        st.metric(
            "P√©nalit√© validation crois√©e",
            f"{cross_penalty:.1f}%",
            delta=f"-{cross_penalty:.1f}",
            delta_color="inverse",
            help="Incoh√©rences entre documents (40% du score final)"
        )
    
    st.markdown("---")
    
    # Analyses d√©taill√©es
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üìà Score par document")
        
        doc_scores = []
        for doc_key, doc_data in st.session_state.analysis_results['documents'].items():
            doc_score = doc_data.get('validation', {}).get('score_fraude', 0) * 100
            risk_level = doc_data.get('validation', {}).get('risk_level', 'Inconnu')
            
            doc_scores.append({
                'Document': doc_key.replace('_', ' ').title(),
                'Score (%)': doc_score,
                'Risque': risk_level
            })
        
        df_scores = pd.DataFrame(doc_scores)
        
        # Graphique
        st.bar_chart(df_scores.set_index('Document')['Score (%)'])
        
        # Tableau
        st.dataframe(
            df_scores.style.background_gradient(subset=['Score (%)'], cmap='RdYlGn_r'),
            use_container_width=True,
            hide_index=True
        )
    
    with col2:
        st.markdown("### üîÑ R√©sultats validation crois√©e")
        
        cross_val = st.session_state.analysis_results.get('cross_validation', {})
        checks = cross_val.get('checks', {})
        
        if checks:
            for check_name, check_value in checks.items():
                check_label = check_name.replace('_', ' ').title()
                
                if check_value is True:
                    st.success(f"‚úÖ {check_label}")
                elif check_value is False:
                    st.error(f"‚ùå {check_label}")
                else:
                    st.info(f"‚ÑπÔ∏è {check_label}")
        
        st.markdown("---")
        
        # Statistiques validation crois√©e
        total_checks = len(checks)
        passed = sum(1 for v in checks.values() if v is True)
        failed = sum(1 for v in checks.values() if v is False)
        
        st.metric("Taux de coh√©rence", f"{(passed/total_checks*100):.0f}%" if total_checks > 0 else "N/A")
    
    # Anomalies globales
    st.markdown("---")
    st.markdown("### ‚ö†Ô∏è Synth√®se des anomalies")
    
    all_anomalies = cross_val.get('anomalies', [])
    
    # Comptage total anomalies
    doc_anomalies_count = sum(
        len(doc.get('validation', {}).get('anomalies', []))
        for doc in st.session_state.analysis_results['documents'].values()
    )
    
    total_anomalies = doc_anomalies_count + len(all_anomalies)
    
    if total_anomalies > 0:
        st.error(f"üö® **{total_anomalies} anomalie(s) au total** : {doc_anomalies_count} dans les documents + {len(all_anomalies)} en validation crois√©e")
    else:
        st.success("‚úÖ **Aucune anomalie d√©tect√©e**")
    
    if all_anomalies:
        st.markdown("**Anomalies de coh√©rence globale :**")
        for anomaly in all_anomalies:
            st.markdown(f"""
            <div class="alert-box alert-warning">
                üîç {anomaly}
            </div>
            """, unsafe_allow_html=True)
    
    # Recommandations d√©taill√©es
    st.markdown("---")
    st.markdown("### üí° Recommandations d'action")
    
    if score < 15:
        st.markdown("""
        <div class="alert-box alert-success">
        <h4>‚úÖ DOSSIER VALID√â - RISQUE TR√àS FAIBLE</h4>
        
        **Analyse** : Le dossier pr√©sente une excellente coh√©rence et authenticit√©. Tous les documents 
        semblent l√©gitimes et les informations sont coh√©rentes entre elles.
        
        **Actions sugg√©r√©es** :
        - ‚úÖ Approuver le dossier sans r√©serve
        - ‚úÖ Poursuivre le processus de location normalement
        - ‚ÑπÔ∏è Archiver le rapport d'analyse
        </div>
        """, unsafe_allow_html=True)
        
    elif score < 30:
        st.markdown("""
        <div class="alert-box alert-success">
        <h4>‚úÖ DOSSIER ACCEPTABLE - RISQUE FAIBLE</h4>
        
        **Analyse** : Le dossier pr√©sente quelques points d'attention mineurs mais reste globalement fiable. 
        Les anomalies d√©tect√©es sont de faible importance.
        
        **Actions sugg√©r√©es** :
        - ‚úÖ Approuver le dossier
        - ‚ö†Ô∏è V√©rifier rapidement les points signal√©s
        - ‚ÑπÔ∏è Conservation d'une vigilance de routine
        </div>
        """, unsafe_allow_html=True)
        
    elif score < 50:
        st.markdown("""
        <div class="alert-box alert-warning">
        <h4>‚ö†Ô∏è VIGILANCE REQUISE - RISQUE MOD√âR√â</h4>
        
        **Analyse** : Le dossier pr√©sente plusieurs anomalies qui n√©cessitent une v√©rification approfondie. 
        Des incoh√©rences ont √©t√© d√©tect√©es mais ne sont pas r√©dhibitoires.
        
        **Actions sugg√©r√©es** :
        - üîç Examiner manuellement les documents signal√©s
        - üìû Contacter le candidat pour clarifications
        - üìß Demander des justificatifs compl√©mentaires si n√©cessaire
        - ‚è∏Ô∏è Suspendre temporairement la validation en attendant √©claircissements
        </div>
        """, unsafe_allow_html=True)
        
    elif score < 70:
        st.markdown("""
        <div class="alert-box alert-danger">
        <h4>üî¥ SUSPICION DE FRAUDE - RISQUE √âLEV√â</h4>
        
        **Analyse** : Le dossier pr√©sente de nombreuses anomalies importantes sugg√©rant une possible 
        falsification de documents. Une investigation approfondie est indispensable.
        
        **Actions sugg√©r√©es** :
        - üö® Ne PAS approuver le dossier en l'√©tat
        - üìû Entretien obligatoire avec le candidat
        - üìÑ Demander les originaux de tous les documents suspects
        - üîç V√©rifier directement aupr√®s des √©metteurs (employeur, DGFiP, etc.)
        - ‚öñÔ∏è Envisager une proc√©dure de signalement si fraude av√©r√©e
        </div>
        """, unsafe_allow_html=True)
        
    else:
        st.markdown("""
        <div class="alert-box alert-danger" style="background: linear-gradient(135deg, #fee2e2 0%, #fca5a5 100%);">
        <h4>üö® FRAUDE PROBABLE - RISQUE TR√àS √âLEV√â</h4>
        
        **Analyse** : Le dossier pr√©sente un nombre critique d'anomalies et d'incoh√©rences. 
        La probabilit√© de fraude documentaire est tr√®s √©lev√©e.
        
        **Actions OBLIGATOIRES** :
        - ‚ùå REJETER le dossier imm√©diatement
        - üö® Ne proc√©der √† AUCUNE validation
        - üìã Documenter pr√©cis√©ment toutes les anomalies
        - ‚öñÔ∏è Signaler le cas aux autorit√©s comp√©tentes si applicable
        - üîí Archiver le dossier pour r√©f√©rence future
        - üìß Informer le service juridique si n√©cessaire
        </div>
        """, unsafe_allow_html=True)


def page_rapport():
    """Page de g√©n√©ration et export du rapport professionnel"""
    
    st.markdown("## üìë Rapport d'Analyse D√©taill√©")
    
    if not st.session_state.analysis_results:
        st.warning("‚ö†Ô∏è Aucune analyse disponible. Effectuez d'abord l'analyse des documents.")
        return
    
    st.markdown("""
    <div class="info-box">
    üìä <strong>Export professionnel</strong><br>
    G√©n√©rez un rapport complet au format Excel pour archivage et transmission.
    Le rapport contient : synth√®se globale, analyse par document, anomalies d√©tect√©es et v√©rifications effectu√©es.
    </div>
    """, unsafe_allow_html=True)
    
    # Aper√ßu du rapport
    st.markdown("### üìÑ Aper√ßu du rapport")
    
    global_score = st.session_state.analysis_results.get('global_score', {})
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Score de fraude", f"{global_score.get('score', 0):.1f}%")
    with col2:
        st.metric("Documents analys√©s", len(st.session_state.analysis_results.get('documents', {})))
    with col3:
        total_anomalies = sum(
            len(doc.get('validation', {}).get('anomalies', []))
            for doc in st.session_state.analysis_results['documents'].values()
        )
        st.metric("Anomalies totales", total_anomalies)
    
    st.markdown("---")
    
    # Section d'export
    st.markdown("### üì• G√©n√©ration et t√©l√©chargement")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        **Le rapport Excel comprend :**
        - üìä Feuille 1 : R√©sum√© global avec verdict et recommandations
        - üìÑ Feuille 2 : Analyse d√©taill√©e de chaque document
        - ‚ö†Ô∏è Feuille 3 : Liste compl√®te des anomalies d√©tect√©es
        - ‚úÖ Feuille 4 : R√©sultats de toutes les v√©rifications
        """)
    
    with col2:
        if st.button("üìä G√âN√âRER LE RAPPORT EXCEL", type="primary", use_container_width=True):
            with st.spinner("‚è≥ G√©n√©ration du rapport en cours..."):
                
                # G√©n√©ration du fichier Excel
                excel_file = create_excel_report(st.session_state.analysis_results)
                
                # Nom du fichier
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"Rapport_AntiFraude_Inli_{timestamp}.xlsx"
                
                st.success("‚úÖ Rapport g√©n√©r√© avec succ√®s !")
                
                # Bouton de t√©l√©chargement
                st.download_button(
                    label="üì• T√©l√©charger le rapport Excel",
                    data=excel_file,
                    file_name=filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
    
    st.markdown("---")
    
    # Donn√©es JSON (pour les utilisateurs avanc√©s)
    with st.expander("üîß Donn√©es brutes (JSON) - Pour utilisateurs avanc√©s", expanded=False):
        st.caption("Donn√©es compl√®tes au format JSON pour traitement automatis√©")
        st.json(st.session_state.analysis_results)
        
        # Export JSON
        json_str = json.dumps(st.session_state.analysis_results, indent=2, ensure_ascii=False)
        st.download_button(
            label="üì• T√©l√©charger JSON",
            data=json_str,
            file_name=f"analyse_fraude_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )


if __name__ == "__main__":
    main()
